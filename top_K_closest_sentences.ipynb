{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD+h1Wwc4/WRifj/UULJiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farnaz-frd/Closest_sentences-from-different-languages-with-laser-embedding/blob/main/top_K_closest_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laser embedding with cosine similarity"
      ],
      "metadata": {
        "id": "PPZjYoqcQx3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing require liberaries"
      ],
      "metadata": {
        "id": "aA1sNuRqQnng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install laserembeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QJkLcEfnWpF",
        "outputId": "793dbb30-48e5-4506-c96b-d1a0fa28b1af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting laserembeddings\n",
            "  Using cached laserembeddings-1.1.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from laserembeddings) (1.22.4)\n",
            "Collecting transliterate==1.10.2\n",
            "  Downloading transliterate-1.10.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.8/859.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt<0.4.0,>=0.3.6\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting torch<2.0.0,>=1.0.1.post2\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.35->laserembeddings) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.35->laserembeddings) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.35->laserembeddings) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.35->laserembeddings) (4.65.0)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.0.1.post2->laserembeddings) (67.7.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=f8cc5889fd4f6871ca9f3cbe3070396bb37be6abbaa8ea780e05bd0b152ce751\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/12/4b/5c9eeed3636a4041c004e859e03429a49105672c7fb09ba6d9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: transliterate, sacremoses, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, mock, subword-nmt, nvidia-cudnn-cu11, torch, laserembeddings\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed laserembeddings-1.1.2 mock-5.0.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 sacremoses-0.0.35 subword-nmt-0.3.8 torch-1.13.1 transliterate-1.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJFSurc6uK2",
        "outputId": "e4f1b450-78cc-490c-af2b-27f2282659fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading models into /usr/local/lib/python3.10/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsvpOL_T62C6",
        "outputId": "3fde6277-194c-434a-aa33-2835697988e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de.unpar.txt  pt.unpar.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main part"
      ],
      "metadata": {
        "id": "3SH2iPCvRDPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VKi0gui4miD5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from laserembeddings import Laser\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "# Load the sentences from the files\n",
        "with open('de.unpar.txt', 'r') as f:\n",
        "    de_sentences = f.read().splitlines()\n",
        "with open('pt.unpar.txt', 'r') as f:\n",
        "    pt_sentences = f.read().splitlines()\n",
        "\n",
        "# Compute the embeddings\n",
        "de_embeddings = laser.embed_sentences(de_sentences, lang='de')\n",
        "pt_embeddings = laser.embed_sentences(pt_sentences, lang='pt')\n",
        "\n",
        "# Compute the cosine similarity\n",
        "similarity = torch.cosine_similarity(torch.tensor(de_embeddings).unsqueeze(1), torch.tensor(pt_embeddings).unsqueeze(0), dim=2)\n",
        "\n",
        "# Get the top K similar sentences\n",
        "K = 100\n",
        "top_K_indices = torch.topk(similarity.flatten(), K).indices\n",
        "top_K_indices = top_K_indices.tolist()\n",
        "top_K_indices = [(i // similarity.shape[1], i % similarity.shape[1]) for i in top_K_indices]\n",
        "\n",
        "# Write the results to a file\n",
        "with open('output.txt', 'w') as f:\n",
        "    for i, j in top_K_indices:\n",
        "        f.write(f'{de_sentences[i]}\\t{pt_sentences[j]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laser embedding with The approximate nearest neighbor search algorithm"
      ],
      "metadata": {
        "id": "E9E6zY9qROj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing require liberaries"
      ],
      "metadata": {
        "id": "nbPLgqH4RaeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c_GB3C8QfHb",
        "outputId": "4e8ba17b-50ba-4e4d-8f4c-e1f402728971"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main part\n"
      ],
      "metadata": {
        "id": "F-HvO6UERnUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from laserembeddings import Laser\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "# Load the sentences from the files\n",
        "with open('de.unpar.txt', 'r') as f:\n",
        "    de_sentences = f.read().splitlines()\n",
        "with open('pt.unpar.txt', 'r') as f:\n",
        "    pt_sentences = f.read().splitlines()\n",
        "\n",
        "# Compute the embeddings\n",
        "de_embeddings = laser.embed_sentences(de_sentences, lang='de')\n",
        "pt_embeddings = laser.embed_sentences(pt_sentences, lang='pt')\n",
        "\n",
        "# Normalize the embeddings\n",
        "de_embeddings = de_embeddings / np.linalg.norm(de_embeddings, axis=1, keepdims=True)\n",
        "pt_embeddings = pt_embeddings / np.linalg.norm(pt_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Build a faiss index\n",
        "d = de_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(pt_embeddings.astype('float32'))\n",
        "\n",
        "# Find the top k most similar pairs of sentences\n",
        "K = 100\n",
        "D, I = index.search(de_embeddings.astype('float32'), K)\n",
        "\n",
        "# Write the results to a file\n",
        "with open('output2.txt', 'w') as f:\n",
        "    for i in range(K):\n",
        "        f.write(f'{de_sentences[I[i][0]]}\\t{pt_sentences[I[i][0]]}\\n')"
      ],
      "metadata": {
        "id": "65Pf_btmn8-2"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}